---
title: "ML HW3"
author: "Josh Elder and Curt Ginder"
date: "3/1/2018"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1: KDD Cup 2009, Customer Relationship Prediction

First we'll pull in the data.

```{r}
PackageList =c('MASS','gbm','tree','randomForest','rpart','caret','ROCR','readxl','data.table','R.utils') 
NewPackages=PackageList[!(PackageList %in% 
                            installed.packages()[,"Package"])]
if(length(NewPackages)) install.packages(NewPackages)
lapply(PackageList,require,character.only=TRUE)#array function



gitURL="https://github.com/ChicagoBoothML/MLClassData/raw/master/KDDCup2009_Customer_relationship/";
DownloadFileList=c("orange_small_train.data.gz","orange_small_train_appetency.labels.txt",
                   "orange_small_train_churn.labels.txt","orange_small_train_upselling.labels.txt")
LoadFileList=c("orange_small_train.data","orange_small_train_appetency.labels.txt",
               "orange_small_train_churn.labels.txt","orange_small_train_upselling.labels.txt")

for (i in 1:length(LoadFileList)){
  if (!file.exists(LoadFileList[[i]])){
    if (LoadFileList[[i]]!=DownloadFileList[[i]]) {
      download.file(paste(gitURL,DownloadFileList[[i]],sep=""),destfile=DownloadFileList[[i]])
      gunzip(DownloadFileList[[i]])
    }else{
      download.file(paste(gitURL,DownloadFileList[[i]],sep=""),destfile=DownloadFileList[[i]])}}
}

na_strings <- c('',
                'na', 'n.a', 'n.a.',
                'nan', 'n.a.n', 'n.a.n.',
                'NA', 'N.A', 'N.A.',
                'NaN', 'N.a.N', 'N.a.N.',
                'NAN', 'N.A.N', 'N.A.N.',
                'nil', 'Nil', 'NIL',
                'null', 'Null', 'NULL')

X=as.data.table(read.table('orange_small_train.data',header=TRUE,
                           sep='\t', stringsAsFactors=TRUE, na.strings=na_strings))
Y_churn    =read.table("orange_small_train_churn.labels.txt", quote="\"")
Y_appetency=read.table("orange_small_train_appetency.labels.txt", quote="\"")
Y_upselling=read.table("orange_small_train_upselling.labels.txt", quote="\"")
```

### Cleaning the Data

Next, we need to clean the data. This will include 4 steps: 1) Removing variables with a high percentage of missing values, 2) Creating a new factor level for remaining missing values, 3) Combining levels to increase the number of data points within categories, and 4) Replacing the remaining missing values among numerical variables. 

To start, we will examine the number of missing values within each predictor variable.

```{r}
# Calculate # of na values for each variable
na_count <-sapply(X, function(y) sum(length(which(is.na(y))))) 
na_count

plot(na_count)
```

Clearly, we want to remove variables that have missing values for a high percentage of observations. For purposes of the analysis, we will set a cutoff that variables must not have more than 50% of observations with a missing entry. We will remove all variables with >50% missing values from the data set. 

```{r}
# Choose cutoff value for deleting variables
n = nrow(X) # number of instances in training set
cut = 0.5*n # identify variables where 50% or more of instances are NA
p = length(X) # 230 variables

# Remove variables where > 50% of values are NA
cut_list = na_count[na_count > cut] # identify variables
exclude_var = names(cut_list) # list names
XS=X[,!(names(X) %in% exclude_var),with=FALSE]
names(XS)

na_count_xs <-sapply(XS, function(y) sum(length(which(is.na(y)))))
plot(na_count_xs)
na_count_xs

length(XS) # 69 variables remain
```

The number of variables has now declined from 230 to 69. Next, we want to create a new factor level for missing values. 

```{r}
## Create a new NA level for factor variables
p = length(XS)

# Before loop
is.factor(XS[[66]]) # variable 66 is a factor variable
length(which(is.na(XS[[66]]))) # variable 66 has 5211 NA values
levels(XS[[66]]) # NA is not currently represented as a level

for (i in 1:p) {
  if (is.factor(XS[[i]])){                   #Isolate factor variables
    CurrentColumn=XS[[i]]                    #Extraction of current column
    idx=is.na(CurrentColumn)                 #Locate the NAs
    CurrentColumn=as.character(CurrentColumn)#Convert from factor to characters
    CurrentColumn[idx]=paste(i,'_NA',sep="") #Add the new NA level strings
    CurrentColumn=as.factor(CurrentColumn)   #Convert back to factors
    XS[[i]]=CurrentColumn
  }
}

# Check that loop works
is.factor(XS[[66]]) # variable 66 is still a factor variable
length(which(is.na(XS[[66]]))) # 0 NA values
levels(XS[[66]]) # New Level was created
table(XS[[66]]) # New level has 5211 values

is.factor(XS[[61]])
length(which(is.na(XS[[61]]))) # 0 NA values
levels(XS[[61]]) # New Level was created
table(XS[[61]]) # New level has 703 values
```

Now, we realize that some factor variables may have levels with limited data points, which could introduce a lot of variability into the prediction on a validation set. Thus, we'll combine levels with fewer data points in order to increase the minimum number of data points within a given level. 

```{r}
## Combine Levels to reduce total number of levels within a factor variable

Thres_Low=249;      #Low Bucket
Thres_Medium=499;   #Medium Bucket
Thres_High=999;     #Large bucket

# Before loop
levels(XS[[69]]) # 30 levels
table(XS[[69]]) # multiple levels with minimal data points

for (i in 1:p) {
  if (is.factor(XS[[i]])){                   #Isolate factor variables
    CurrentColumn=XS[[i]]                    #Extraction of current column
    CurrentColumn_Table=table(CurrentColumn) #Tabulate the frequency
    levels(CurrentColumn)[CurrentColumn_Table<=Thres_Low]=paste(i,'_Low',sep="")
    CurrentColumn_Table=table(CurrentColumn) #Tabulate the new frequency 
    levels(CurrentColumn)[CurrentColumn_Table>Thres_Low & CurrentColumn_Table<=Thres_Medium ]=paste(i,'_Medium',sep="")
    CurrentColumn_Table=table(CurrentColumn) #Tabulate the new frequency
    levels(CurrentColumn)[CurrentColumn_Table>Thres_Medium & CurrentColumn_Table<=Thres_High ]=paste(i,'_High',sep="")
    XS[[i]]=CurrentColumn
  }
}

# After loop
levels(XS[[69]]) # 9 levels

```

Finally, we need to clean the missing values in our numeric variables. For this analysis, we will replace missing values with the average of the observed values within the given variable. 

```{r}
## Replace NA values within numeric variables with the average 

# Check a numeric variable column with NA's
na_count_xs
class(XS[[1]]) # variable 1 is an integer
is.numeric(XS[[1]]) # true
mean(XS[[1]]) #NA
mean(XS[[1]][!is.na(XS[[1]])]) # 1326.437

for (i in 1:p) {
  if (is.numeric(XS[[i]])){
    CurrentColumn=XS[[i]]
    CurrentColumn[is.na(CurrentColumn)] <- mean(CurrentColumn[!is.na(CurrentColumn)])
    XS[[i]] = CurrentColumn
  }
}

# Check that loop is working
mean(XS[[1]]) # 1326.437
mean(XS[[2]]) # 1326.437

na_count_fin <-sapply(XS, function(y) sum(length(which(is.na(y))))) 
plot(na_count_fin)

# no more variables with NA's left
```

The plot illustrates that there are no numeric variables remaining with missing values.

### Feature / Variable Selection

We've now cleaned the data set and removed variables with a lot of missing values. Next we want to further narrow the field of prediction variables down from the 69 that are currently in our data set. 

For purposes of our analysis, we are going to focus on predicting customer churn. We'll combine the X and Y variables into a single data frame, and reclassify Churn as a Factor Variable. 

```{r}
churn.df = cbind(XS,Y_churn)
str(churn.df)
hist(churn.df$V1)
churn.df$V1 = as.factor(churn.df$V1)

#Re-name levels
levels(churn.df$V1)[levels(churn.df$V1)=="-1"] <- "No Churn"
levels(churn.df$V1)[levels(churn.df$V1)=="1"] <- "Churn"
levels(churn.df$V1)
table(churn.df$V1)
```

We see that the propensity for no churn is much higher than churn. This suggests that when we train our model and predict on the validation set, the misclassification rate may not be the best performance measure. 

We'll now seperate the data into training and test sets, and then create a smaller subset of training data for which we can use for variable selection. 

```{r}
# Separate data into training and test sets
set.seed(1)
train = sample.int(n, floor(0.80*n))
xtrain = churn.df[train,] #training set with 80% of training samples
xtest = churn.df[-train,] #test set with 20% of training samples

# Create a smaller training subset for variable selection
set.seed(1)
train = sample.int(nrow(xtrain), floor(0.25*nrow(xtrain)))
xvs = xtrain[train,] #1000 observations
```

For variable selection, we will run a random forest model and examine the variable importance measures. 

```{r}
# Run a random forest model and check variable importance
p = length(xvs)

rf = randomForest(xvs$V1~.,              #regression model
                   data=xvs, #data set
                   mtry=sqrt(p),     #number of variables to sample
                   ntree=1000,  #number of trees to grow
                   importance=TRUE#calculate variable importance measure (optional)
)

varImpPlot(rf)

var_imp = data.frame(rf$importance)
var_imp = var_imp[order(var_imp[,4], decreasing=TRUE),] #sort data frame by Mean Decrease Gini
hist(var_imp[,4]) #check distribution of variable importance
```

In order to further narrow the field of prediction variables, we'll initiate a cutoff point in which to select the most impactful variables. For this analysis, we'll keep variables with a Mean Decrease in the Gini index greater than 20. As we see below, this leaves us with 31 variables in which to train and validate our model. 

```{r}
var_sel = var_imp[var_imp$MeanDecreaseGini > 20,]
nrow(var_sel) #31 variables

plot(var_sel$MeanDecreaseGini, col="white", main = "RF Variable Importance", ylab = "Mean Decrease Gini")
text(x = 1:31, y =var_sel$MeanDecreaseGini, labels = rownames(var_sel), cex=0.6)
```

Now we will create a new subset of training dat based on the 31 most important variables selected above. 

```{r}
## Create subset of training data frame based on the top 31 variables found from the Random Forest Model

var_names = rownames(var_sel)

xtrain2 = xtrain[,(names(xtrain) %in% var_names),with=FALSE] 
xtrain2$Churn = xtrain$V1

xtest2 = xtest[,(names(xtest) %in% var_names),with=FALSE] 
xtest2$Churn = xtest$V1
```

### Training the Model

Now we are ready to train the model. First, we'll divide our new training subset into a training and validation set, with 25% of the data in the validation set.  

```{r}
## Split training set into training and validation set
set.seed(1)
train = sample.int(nrow(xtrain2), floor(0.75*nrow(xtrain2)))
xtrain3 = xtrain2[train,] #training set with 75% of training samples
xval = xtrain2[-train,] #test set with 25% of training samples
```

We'll initiate loss functions in which to measure the model performance.

```{r}
# define loss function
# deviance loss function
# y should be 0/1
# phat are probabilities obtain by our algorithm 
# wht shrinks probs in phat towards .5 --- this helps avoid numerical problems don't use log(0)!
lossf = function(y,phat,wht=0.0000001) {
  if(is.factor(y)) y = as.numeric(y)-1
  phat = (1-wht)*phat + wht*.5
  py = ifelse(y==1, phat, 1-phat)
  return(-2*sum(log(py)))
}

# deviance loss function
# y should be 0/1
# phat are probabilities obtain by our algorithm 
# thr is the cut off value - everything above thr is classified as 1
getConfusionMatrix = function(y,phat,thr=0.5) {
  # some models predict probabilities that the data belongs to class 1,
  # This function convert probability to 0 - 1 labels
  if(is.factor(y)) y = as.numeric(y)-1
  yhat = ifelse(phat > thr, 1, 0)
  tb = table(predictions = yhat, 
             actual = y)  
  rownames(tb) = c("predict_0", "predict_1")
  return(tb)
}

# deviance loss function
# y should be 0/1
# phat are probabilities obtain by our algorithm 
# thr is the cut off value - everything above thr is classified as 1
lossMR = function(y,phat,thr=0.5) {
  if(is.factor(y)) y = as.numeric(y)-1
  yhat = ifelse(phat > thr, 1, 0)
  return(1 - mean(yhat == y))
}

# Initiate Lists
phatV = list()
phatT = list()
```

Now we will train random forests and boosting models, evaluating various model parameters against their prediction performance on the validation set. 

First, random forests.

```{r}
## Random Forests

ntrain = nrow(xtrain3)
nval = nrow(xval)
ntest = nrow(xtest2)
p = length(xtrain3)

# Initiate Parameters
m = round(sqrt(p)-1,0)
ntree = c(100, 500, 1000, 2000)
nodesize = c(10,20)
rf_params = expand.grid(m, ntree, nodesize)
colnames(rf_params) = c("Variables", "Trees", "Node Size")
phatV$rf = matrix(0.0, nval, nrow(rf_params))
phatT$rf = matrix(0.0, ntest, nrow(rf_params))

for (i in 1:nrow(rf_params)) {
  frf = randomForest(xtrain3$Churn~., data=xtrain3, #data set
                     mtry=m,     #number of variables to sample
                     ntree=rf_params[i,2],  #number of trees to grow
                     nodesize = rf_params[i,3])
  phat = predict(frf, newdata = xval, type="prob")[,2]
  phatV$rf[,i]=phat
  print(i)
}
```

Next we will also train boosting models across various parameters. 

```{r}
## Boosting

## Convert Churn back to a numerical value with 0 and 1

xtrainB = xtrain3
xtrainB$Churn = as.numeric(xtrainB$Churn) - 1
xvalB = xval
xvalB$Churn = as.numeric(xval$Churn) - 1

hist(xtrainB$Churn)
hist(xvalB$Churn)

tree_depth = c(5, 10) 
tree_num = c(500, 1000, 2000) 
lambda=c(0.1, 0.01) 
boosting.params = expand.grid(tree_depth,tree_num,lambda) 
colnames(boosting.params) = c("tdepth","ntree","shrink")
phatV$boost = matrix(0.0, nval, nrow(boosting.params))  # we will store results here

set.seed(1)
for(i in 1:nrow(boosting.params)) { 
  boost_fit =gbm(xtrainB$Churn~.,data=xtrainB,distribution="bernoulli", 
                 interaction.depth=boosting.params[i,1],
                 n.trees=boosting.params[i,2],
                 shrinkage=boosting.params[i,3]) 
  phat = predict(boost_fit,
                 newdata=xvalB,
                 n.trees=boosting.params[i,2],
                 type="response") 
  phatV$boost[,i] = phat
  print(i)
} 
```

Now that we have trained our models, we will evaluate their respective performances. First, we will plot and calculate their deviance losses. 

```{r}
lossL = list() 
nmethod = length(phatV) 
for(i in 1:nmethod) { 
  nrun = ncol(phatV[[i]]) 
  lvec = rep(0,nrun) 
  for(j in 1:nrun) 
    lvec[j] = lossf(xval$Churn, phatV[[i]][,j]) 
  lossL[[i]]=lvec; names(lossL)[i] = names(phatV)[i] 
} 
lossv = unlist(lossL)

# Plot the deviance loss for our models on the validation set
par(mfrow=c(1,1)) 
plot(lossv, ylab="loss on validation", type="n") 
nloss=0 
for(i in 1:nmethod) { 
  ii = nloss + 1:ncol(phatV[[i]]) 
  points(ii,lossv[ii],col=i,pch=17) 
  nloss = nloss + ncol(phatV[[i]]) } 
legend("topright",legend=names(phatV),col=1:nmethod,pch=rep(17,nmethod))
```

Now we'll check which model minimized the deviance loss on the validation set. 

```{r}
# Check which one is the minimum
which.min(lossv) #Boost 9
boosting.params[9,]
# Parameters: Depth = 5, Trees = 1000, Shrinkage = 0.01

```

The boosting model that minimizes the the deviance loss is: ntrees = 1000, tdepth = 5, shrinkage = 0.01. 

### Measuring performance on test set

We'll now retrain the boosting model with the entire training set, and then measure the performance against the testing set. 

```{r}
## Re-fit RF to entire training + validation set

xtrainB2 = xtrain2
xtrainB2$Churn = as.numeric(xtrain2$Churn) - 1
xtestB = xtest2
xtestB$Churn = as.numeric(xtestB$Churn) - 1

boost_best = gbm(xtrainB2$Churn~.,data=xtrainB2,distribution="bernoulli", 
                 interaction.depth=5,
                 n.trees=1000,
                 shrinkage=0.01)


phat_btest = predict(boost_best,
                             newdata=xtestB,
                             n.trees=1000,
                             type="response") 
```

Now we'll calculate the deviance loss and misclassification rate on the test set. 

```{r}
# Get confusion matrix on test set
getConfusionMatrix(xtestB$Churn, phat_btest, 0.5)
lossMR(xtestB$Churn, phat_btest, 0.5)
# 7.3% misclassificaiton

# Loss on test test
lossf(xtestB$Churn, phat_btest)
# 4883
```

The misclassification rate is 7.3%, and the deviance loss is 4883. However, we can see that the model suffers from a high false negative rate. Of the 735 positive instances of churn, our model only correctly predicted 11 of them (using a 0.5 cutoff). Part of the challenge is driven by the unbalanced data set, wherein only 7.35% of the total observations are positive churn.  

If we were to implement the model to predict which customers are likely to churn, we could attach expected values to the rates in order to determine at what probability of churn we would want to pay attention.  

Shown below is the ROC curve for the model. 

```{r}
# ROC Curves for Boosting Model
plot(c(0,1), c(0,1), xlab ="FPR", ylab="TPR", main="ROC Curve: Boosting Model", cex.lab=1, type="n")
pred = prediction(phat_btest, xtestB$Churn)
perf = performance(pred, measure = "tpr", x.measure = "fpr")
lines(perf@x.values[[1]], perf@y.values[[1]],col=1)
abline(0,1, lty=2)
```

## Question 2: Physical Activity Classificaiton

First, we'll pull in the data and load h2o. 

```{r}
download.file(
    paste("https://raw.githubusercontent.com/ChicagoBoothML/MLClassData/master/",
          "HumanActivityRecognitionUsingSmartphones/ParseData.R",sep=""),
    "ParseData.R")
source("ParseData.R")

# load data
data = parse_human_activity_recog_data()

###### fun starts here

library(h2o)

# start h2o server
h2oServer = h2o.init(nthreads = -1)

# load data into h2o format
Xtrain = as.h2o(data$X_train, destination_frame = "Xtrain")
Ytrain = as.h2o(data$y_train, destination_frame = "Ytrain")
Xtest = as.h2o(data$X_test, destination_frame = "Xtest")
Ytest = as.h2o(data$y_test, destination_frame = "Ytest")
```

Next, we'll train a simple neural network and calculate the misclassification rates. 

```{r}
# train a simple neural network
p = ncol(Xtrain)
simpleNN = h2o.deeplearning(x=1:p, y=p+1,     # specify which columns are features and which are target
                            training_frame = h2o.cbind(Xtrain, Ytrain),   # combine features with labels
                            hidden = 10,      # 1 hidden layer with 10 neurons
                            epochs = 5,       # this is a test run, so 5 epochs is fine
                            model_id = "simple_nn_model"
                            )

phat = h2o.predict(simpleNN, Xtest) # compute probabilities for new data 

o = h2o.confusionMatrix(simpleNN, h2o.cbind(Xtest, Ytest)) # compute confusion matrix
o
```

The total error is 6.4%.

A slightly more complicated with two hidden layers...

```{r}
simpleDL = h2o.deeplearning(x=1:p, y=p+1,     # specify which columns are features and which are target
                            training_frame = h2o.cbind(Xtrain, Ytrain),   # combine features with labels
                            hidden = c(50, 20),      # 2 hidden layers with 50 and 20 neurons 
                            epochs = 5,       # this is a test run, so 5 epochs is fine
                            l1 = 1e-5,          # regularize
                            model_id = "simple_dl_model"
                            )
  
h2o.confusionMatrix(simpleDL, h2o.cbind(Xtest, Ytest)) # compute confusion matrix
```

We see this model has a total error of 7.0%, a decline over the previous model. 

Next we also train a simple random forest and boosting model.

```{r}
# random forest
rf.model = h2o.randomForest(x=1:p, y=p+1,     # specify which columns are features and which are target
                            training_frame = h2o.cbind(Xtrain, Ytrain),   # combine features with labels
                            ntrees = 5,                                   # this poor forest only has 5 trees
                            min_rows = 20,                                # each leaf needs to have at least 20 nodes
                            max_depth = 10,                               # we do not want too deep trees
                            model_id = "simple_rf_model"
                            )

h2o.confusionMatrix(rf.model, h2o.cbind(Xtest, Ytest)) # compute confusion matrix

# boosting

gbm.model =  h2o.gbm(x=1:p, y=p+1,     # specify which columns are features and which are target
                     training_frame = h2o.cbind(Xtrain, Ytrain),   # combine features with labels
                     ntrees = 5,                                   # this poor boosting model only has 5 trees
                     min_rows = 20,                                # each leaf needs to have at least 20 nodes
                     max_depth = 3,                                # we want shallow trees
                     model_id = "simple_gbm_model"
                     )

h2o.confusionMatrix(gbm.model, h2o.cbind(Xtest, Ytest)) # compute confusion matrix
```

The random forest model has a total error rate of 8.3% and the boosting model has a total error rate of 12.7%.

### Experiment with additional model variations.

Now we'll try a 3 hidden layer neural net to see if adding model complexity improves the performance.

```{r}
# Try 3 layers
set.seed(1)
DL1 = h2o.deeplearning(x=1:p, y=p+1,     # specify which columns are features and which are target
                       training_frame = h2o.cbind(Xtrain, Ytrain),   # combine features with labels
                       hidden = c(10,10,10),  # 3 layers with 10 neurons each     
                       epochs = 5,       
                       l1 = 1e-5,          # regularize
                       model_id = "dl1_model"
)

h2o.confusionMatrix(DL1, h2o.cbind(Xtest, Ytest)) # compute confusion matrix
# 6.6%

```

The 3 layer model with 10 neurons at each layer has an error rate of 6.6%. This shows no improvement over the simplier model with 1 layer and 10 neurons. Now we'll try adding more neurons at each hidden layer. 

```{r}
# More neurons per layer
set.seed(1)
DL2 = h2o.deeplearning(x=1:p, y=p+1,     # specify which columns are features and which are target
                       training_frame = h2o.cbind(Xtrain, Ytrain),   # combine features with labels
                       hidden = c(50,50,50),  # 3 layers with 50 neurons each       
                       epochs = 5,       
                       l1 = 1e-5,          # regularize
                       model_id = "dl2_model"
)

h2o.confusionMatrix(DL2, h2o.cbind(Xtest, Ytest)) # compute confusion matrix
# 6.7%, too complex

```

Again, we don't see any real improvement. Next we'll try switching the function type from rectifier to tanh.

```{r}
# change function type from rectifier to tanh
set.seed(1)
DL3 = h2o.deeplearning(x=1:p, y=p+1,     # specify which columns are features and which are target
                       training_frame = h2o.cbind(Xtrain, Ytrain),   # combine features with labels
                       hidden = c(10,10,10),  # 3 layers with 10 neurons each       
                       epochs = 5,       
                       l1 = 1e-5,          # regularize
                       activation = "Tanh",
                       model_id = "dl3_model"
)

h2o.confusionMatrix(DL3, h2o.cbind(Xtest, Ytest)) # compute confusion matrix
```

The error rate is unchanged at 6.7%.

Now we'll try looping over various parameters for a single layer neural network, varying 1) the number of neurons, 2) the number of epochs, and 3) the activation function. 

```{r}
# Loop through multiple parameters for a single layer neural net

neurons = c(5,10,20,50)
epoch = c(5,10)
activ = c("Tanh", "TanhWithDropout", "Rectifier", "RectifierWithDropout")
nn.params = expand.grid(neurons,epoch,activ)
colnames(nn.params) = c("Neurons", "Epochs", "Function")
num_params = nrow(nn.params)
error_vec = rep(0,num_params)

set.seed(1)
for(i in 1:num_params) {
  nn = h2o.deeplearning(x=1:p, y=p+1,     # specify which columns are features and which are target
                        training_frame = h2o.cbind(Xtrain, Ytrain),   # combine features with labels
                        hidden = nn.params[i,1],         
                        epochs = nn.params[i,2],
                        activation = as.character(nn.params[i,3])
  )
  conf = h2o.confusionMatrix(nn, h2o.cbind(Xtest, Ytest))
  error_vec[i] = conf$Error[7]
  print(i)         
}

nn.params[which.min(error_vec),]
# Parameters: 10 neurons, 5 epochs, Tanh

error_vec[which.min(error_vec)]
# 4.8% misclassificaiton rate
```

We find that the parameters that minimize the total error rate are 10 neurons, 5 epochs, and the tanh activation function. The misclassification rate is 4.8%. 

Now we will try looping over the number of nuerons and epochs for a 2 layer model, using the rectifier with dropout activation function.

```{r}
# Loop with 2 hidden layers

neurons = c(5,10,20)
epoch = c(5,10)
nn.params = expand.grid(neurons,epoch)
colnames(nn.params) = c("Neurons", "Epochs")
num_params = nrow(nn.params)
error_vec = rep(0,num_params)

set.seed(1)
for(i in 1:num_params) {
  nn = h2o.deeplearning(x=1:p, y=p+1,     # specify which columns are features and which are target
                        training_frame = h2o.cbind(Xtrain, Ytrain),   # combine features with labels
                        hidden = c(nn.params[i,1],nn.params[i,1]),         
                        epochs = nn.params[i,2],
                        activation = "RectifierWithDropout"
  )
  conf = h2o.confusionMatrix(nn, h2o.cbind(Xtest, Ytest))
  error_vec[i] = conf$Error[7]
  print(i)         
}

nn.params[which.min(error_vec),]
# Parameters: 20 neurons, 5 epochs

error_vec[which.min(error_vec)]
# 6.8% misclassificaiton rate
```

With two layers, the optimal parameters are 20 neurons and 5 epochs. The misclassification rate increased to 6.8%. 

Thus, our best neural net model is 10 neurons, 5 epochs, and tanh with a 4.8% misclassificaiton rate.



Finally, we can try a few more boosting models, looping over 1) tree depth and 2) number of trees.

```{r}
#############
# Try more boosting models

tree_depth = c(5,10)
tree_num = c(100, 500, 1000)
boosting_params = expand.grid(tree_depth, tree_num)
colnames(boosting_params) = c("Depth", "Trees")
num_params = nrow(boosting_params)
error_vec = rep(0,num_params)

set.seed(1)
for (i in 1:num_params) {
  gbm.model =  h2o.gbm(x=1:p, y=p+1,     # specify which columns are features and which are target
                     training_frame = h2o.cbind(Xtrain, Ytrain),   # combine features with labels
                     max_depth = boosting_params[i,1],
                     ntrees = boosting_params[i,2]
                     )
  conf = h2o.confusionMatrix(gbm.model, h2o.cbind(Xtest, Ytest))
  error_vec[i] = conf$Error[7]
  print(i)
}

boosting_params[which.min(error_vec),]
# Parameters: depth = 5, trees = 1000

error_vec[which.min(error_vec)]
# 6.4% misclassificaiton rate
```

Our optimal boosting model has tree depth = 5 and number of trees = 1000. The total error rate is 6.4%.

We find that the neural net has greater accuracy than the random forest and boosting models. 