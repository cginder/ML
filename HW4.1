#### HW 4.1 ####
## Libraries
PackageList =c('MASS','gbm','tree','randomForest','rpart','caret','ROCR','readxl','R.utils','data.table',
               'kernlab','ggplot2',"gettingtothebottom")
NewPackages=PackageList[!(PackageList %in%
                            installed.packages()[,"Package"])]
if(length(NewPackages)) install.packages(NewPackages)
lapply(PackageList,require,character.only=TRUE)#array function

# Read in Data
train <- read.csv("HW4_train.csv")
test <- read.csv("HW4_test.csv")

# X&Y Subset
train.x <- as.matrix(train[,c(2:11)])

train.y <- as.vector(train[,c(1)])

#Start stop watch timer
tic <- function(gcFirst = TRUE, type=c("elapsed", "user.self", "sys.self")){
  type <- match.arg(type)
  assign(".type", type, envir=baseenv())
  if(gcFirst) gc(FALSE)
  tic <- proc.time()[type]         
  assign(".tic", tic, envir=baseenv())
  invisible(tic)
}
#Read elapsed time from stopwatch
toc <- function(){
  type <- get(".type", envir=baseenv())
  toc <- proc.time()[type]
  tic <- get(".tic", envir=baseenv())
  print(toc - tic)
  invisible(toc)
}


# objective function
l = function(X, y, b){
    -t(y)%*%(X%*%b) + sum(log(1+exp(X%*%b)))
}

# gradient function
grad_l = function(X, y, b){
  -t(X)%*%(y-plogis(X%*%b))
}

# Run gradient descent
alpha = 0.005
tic()
logistic_gd1 <- gdescent(l, grad_l, train.x, train.y, alpha = alpha, iter=15000)
toc()

# gdescent returns several values, including trace of parameters in all iterations
# see names of returned values
names(logistic_gd)

# Run gradient descent for 3 different alphas
alpha1 = 0.0001
tic()
logistic_gd1 <- gdescent(l, grad_l, train.x, train.y, alpha = alpha1, iter=15000)
toc()

alpha2 = 0.01
tic()
logistic_gd2 <- gdescent(l, grad_l, train.x, train.y, alpha = alpha2, iter=15000)
toc()

alpha3 = 0.15
tic()
logistic_gd3 <- gdescent(l, grad_l, train.x, train.y, alpha = alpha3, iter=15000)
toc()

# Draw Plots of Object Function, Gradient Norm, and a Few Coefficients
par(mfrow = c(1,3))
plot(log10(logistic_gd1$f),type='b',lwd=0.5,col='red', main = paste("Object Funtion; Alpha = ",alpha1,sep=""),
     xlab ="iterations", ylab = "Objective Value (log10-scale)")
plot(log10(logistic_gd2$f),type='b',lwd=0.5,col='red', main = paste("Object Funtion; Alpha = ",alpha2,sep=""),
     xlab ="iterations", ylab = "Objective Value (log10-scale)")
plot(log10(logistic_gd3$f),type='b',lwd=0.5,col='red', main = paste("Object Funtion; Alpha = ",alpha3,sep=""),
     xlab ="iterations", ylab = "Objective Value (log10-scale)")


## 1.2
# Using reasonable alpha, run gradient descent again
alpha <- alpha2
logistic_gd <- gdescent(l, grad_l, train.x, train.y, alpha = alpha, iter=15000)

conv.iter <- which.min(logistic_gd$f)
min.fun.value <- min(logistic_gd$f)
coef.values <- logistic_gd$b[,conv.iter]
names(coef.values) <- c("Intercept",1:10)

cat(c("The model converges at",conv.iter,"iterations, with a minimum function value of",min.fun.value))
cat(c("Coefficient Values: "))
print(coef.values)

# Compare to Built in GLM() function
summary(glm(train.y ~ train.x, family = binomial))$coef[,1]


# Calculate predicted probability for each data point in test set
test.x <- as.matrix(test[,2:11])
test.y <- test[,1]

logistic.prediction <- function(x,b){
  obs <- nrow(x)
  pred.y <- c(1:obs)
  for(i in 1:obs){
    pred.y[i] <- (exp(b[1] + sum(x[i,]*b[2:11])))/(1+exp(b[1]+sum(x[i,]*b[2:11])))
  }
  return(pred.y)
}

pred.test <- logistic.prediction(test.x,as.vector(coef.values))

# Misclassifcation Rate
lossMR = function(y,phat,thr=0.5) {
  if(is.factor(y)) y = as.numeric(y)-1
  yhat = ifelse(phat > thr, 1, 0)
  return(1 - mean(yhat == y))
}
# Confusion Matrix
getConfusionMatrix = function(y,phat,thr=0.5) {
  if(is.factor(y)) y = as.numeric(y)-1
  yhat = ifelse(phat > thr, 1, 0)
  tb = table(predictions = yhat, 
             actual = y)  
  rownames(tb) = c("predict_0", "predict_1")
  return(tb)
}

getConfusionMatrix(test.y, pred.test, 0.5)
cat('Missclassification rate = ', lossMR(test.y, pred.test, 0.5), '\n')
