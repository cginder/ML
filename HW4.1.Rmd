---
title: "Machine Learning Homework 4.1"
author: "Josh Elder and Curt Ginder"
header-includes: null
output:
  word_document: default
  pdf_document: default
  html_document:
    df_print: paged
geometry: left=2.5cm,right=2.5cm,top=1cm,bottom=1.5cm
---

First, we read in libraries including the new package "gettingtothebottom"
```{r message=TRUE, include=FALSE}
#### HW 4.1 ####
## Libraries
PackageList =c('MASS','gbm','tree','randomForest','rpart','caret','ROCR','readxl','R.utils','data.table',
               'kernlab','ggplot2',"gettingtothebottom")
NewPackages=PackageList[!(PackageList %in%
                            installed.packages()[,"Package"])]
if(length(NewPackages)) install.packages(NewPackages)
lapply(PackageList,require,character.only=TRUE)#array function
```
Then we read in data, already partitioned into training and test sets.
```{r}
# Read in Data
train <- read.csv("HW4_train.csv")
test <- read.csv("HW4_test.csv")

# X&Y Subset
train.x <- as.matrix(train[,c(2:11)])
train.y <- as.vector(train[,c(1)])
```
For our own curiosity, we used timing functions to evaluate the training time with various alpha values.
```{r}
#Start stop watch timer
tic <- function(gcFirst = TRUE, type=c("elapsed", "user.self", "sys.self")){
  type <- match.arg(type)
  assign(".type", type, envir=baseenv())
  if(gcFirst) gc(FALSE)
  tic <- proc.time()[type]         
  assign(".tic", tic, envir=baseenv())
  invisible(tic)
}
#Read elapsed time from stopwatch
toc <- function(){
  type <- get(".type", envir=baseenv())
  toc <- proc.time()[type]
  tic <- get(".tic", envir=baseenv())
  print(toc - tic)
  invisible(toc)
}
```

## 4.1.1 Implement a logistic regression classifier using the gradient descent by filling in the missing code for the following functions:


We then created our objective function to maximize the likelihood function, which is the equivaent of minimizing the negative log-likelihood.
```{r}
# objective function
l = function(X, y, b){
    -t(y)%*%(X%*%b) + sum(log(1+exp(X%*%b)))
}
```
We then included the gradient function that updates the estimate b.
```{r}
# gradient function
grad_l = function(X, y, b){
  -t(X)%*%(y-plogis(X%*%b))
}
```
## 4.1.2 Use the train set. Choose a reasonable step-size α. Plot the value of the objective function on each iteration of gradient descent, with the iteration number on the horizontal axis and the objective value on the vertical axis. Make sure to include axis labels and a title for your plot. Draw the plot described above for three cases: too small α, reasonable α and too big α. You should be able to see the object function decreases quickly and then becomes stable if the scale of α is reasonable.

We then ran our first gradient descent using an alpha value of 0.005 and 15000 iterations.
```{r}
# Run gradient descent
alpha = 0.005
tic()
logistic_gd1 <- gdescent(l, grad_l, train.x, train.y, alpha = alpha, iter=15000)
toc()

# gdescent returns several values, including trace of parameters in all iterations
# see names of returned values
names(logistic_gd1)
```

We then ran gradient descent for three different alphas. Our intention was to have one alpha that was too small, one reasonable alpha, and one alpha that is too large.

```{r}
# Run gradient descent for 3 different alphas
alpha1 = 0.0001
tic()
logistic_gd1 <- gdescent(l, grad_l, train.x, train.y, alpha = alpha1, iter=15000)
toc()

alpha2 = 0.01
tic()
logistic_gd2 <- gdescent(l, grad_l, train.x, train.y, alpha = alpha2, iter=15000)
toc()

alpha3 = 0.15
tic()
logistic_gd3 <- gdescent(l, grad_l, train.x, train.y, alpha = alpha3, iter=15000)
toc()

# Draw Plots of Object Function, Gradient Norm, and a Few Coefficients
par(mfrow = c(1,3))
plot(log10(logistic_gd1$f),type='b',lwd=0.5,col='red', main = paste("Object Funtion; Alpha = ",alpha1,sep=""),
     xlab ="iterations", ylab = "Objective Value (log10-scale)")
plot(log10(logistic_gd2$f),type='b',lwd=0.5,col='red', main = paste("Object Funtion; Alpha = ",alpha2,sep=""),
     xlab ="iterations", ylab = "Objective Value (log10-scale)")
plot(log10(logistic_gd3$f),type='b',lwd=0.5,col='red', main = paste("Object Funtion; Alpha = ",alpha3,sep=""),
     xlab ="iterations", ylab = "Objective Value (log10-scale)")
```

As we can see, the first alpha (0.0001) does not converge within the 15000 iterations and is too small. The second alpha (0.01) is reasonable, quickly decreasing the objective function and converging at ~4,500 iterations. The final alpha (0.15) is clearly too large, becoming unstable as it does not find the minimum objective value.


##3. Use the reasonable α you get and run the gradient descent again. Report the number of iterations that are required for the algorithm to converge. Report the estimated regression coefficients. Also, implement logistic regression by using the function glm() in R. Compare estimates of your gradient descent and glm() function. Are they the same?

We then turned to using the reasonable alpha (0.01) for further evaluation of gradient descent.

```{r}
# Using reasonable alpha, run gradient descent again
alpha <- alpha2
logistic_gd <- gdescent(l, grad_l, train.x, train.y, alpha = alpha, iter=15000)

#conv.iter <- which.min(logistic_gd$f)
conv.iter <- length(logistic_gd$iterates)
min.fun.value <- min(logistic_gd$f)
coef.values <- logistic_gd$b[,conv.iter]
names(coef.values) <- c("Intercept",1:10)

cat(c("The model converges at",conv.iter,"iterations, with a minimum function value of",min.fun.value,"\n"))
cat(c("Coefficient Values: "))  
print(coef.values)
```

Using an alpha of 0.01, we see that the number of iterations required to converge is 4690 iterations. At the final iteration, we see that the coefficient estimates are as follows:
```{r}
cat(c("Coefficient Values: ","\n"))  
print(coef.values)
```

We then turned to compare these results with the built in glm() function in R.
```{r}
# Compare to Built in GLM() function
glm.coef <- summary(glm(train.y ~ train.x, family = binomial))$coef[,1]

coef.compare <- cbind(Grad.Desc = coef.values,GLM = glm.coef)

print(coef.compare)
```

We see that the coefficients are very similar, although not quite identical. This could be possible attributed to the GLM stopping before the true minimum.

## 4.1.4 Write a function to compute predicted probability for each data point in the test set (use the first equation in this question). Make prediction by using the regression coefficients in the last iteration of the gradient descent. Report misclassification rate on the test set.


We then turned to computing predicted probability for each data point in the test set using the first 

```{r}
# Calculate predicted probability for each data point in test set
test.x <- as.matrix(test[,2:11])
test.y <- test[,1]

logistic.prediction <- function(x,b){
  obs <- nrow(x)
  pred.y <- c(1:obs)
  for(i in 1:obs){
    pred.y[i] <- (exp(b[1] + sum(x[i,]*b[2:11])))/(1+exp(b[1]+sum(x[i,]*b[2:11])))
  }
  return(pred.y)
}

pred.test <- logistic.prediction(test.x,as.vector(coef.values))
```
We then evaluated the misclassification rate in the test set

```{r}
# Misclassifcation Rate
lossMR = function(y,phat,thr=0.5) {
  if(is.factor(y)) y = as.numeric(y)-1
  yhat = ifelse(phat > thr, 1, 0)
  return(1 - mean(yhat == y))
}
# Confusion Matrix
getConfusionMatrix = function(y,phat,thr=0.5) {
  if(is.factor(y)) y = as.numeric(y)-1
  yhat = ifelse(phat > thr, 1, 0)
  tb = table(predictions = yhat, 
             actual = y)  
  rownames(tb) = c("predict_0", "predict_1")
  return(tb)
}

getConfusionMatrix(test.y, pred.test, 0.5)
cat('Misclassification rate = ', lossMR(test.y, pred.test, 0.5), '\n')
```


We find that the model does quite well, with a misclassification rate of 6.5% on the test set.



