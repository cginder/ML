## HW 1
# load KKNN Library for KNN model
library(kknn)

# set seed to maintain reproducibility
set.seed(99)

#### Question 1.1 - Generate Random Data & Linear Model

# create sample data
# sample x
x.train <- rnorm(100,0,1) # 100 random draws with a mean of 0 and a st.d. of 1
# sample error
e.train <- rnorm(100,0,1) # 100 random draws with a mean of 0 and a st.d. of 1 for error term

# y = (1.8 * x + 2 + error)
y.train <- (1.8*x.train + 2 + e.train)

# scatter plot of training set
plot(y.train~x.train)
abline(a = 2, b = 1.8,lwd = 2) # plot a line of what the equation should be (without error)

# create test set 
x.test <- rnorm(10000,0,1) # same thing as above, 10,000 draws this time though
e.test <- rnorm(10000,0,1) # " " " "
y.test <- (1.8*x.test + 2 + e.test)


# Q1.3 Ordinary Linear Regression w/ training data
df.train <- as.data.frame(cbind(x.train,y.train)) # build data frame for data of interest
colnames(df.train)[1:2] <- c("x","y") # rename to standard x and y variables to more easily compare to testing data
lmTrain <- lm(y ~ x,data = df.train) # create a linear regression model for y on x (remember y has error term built in)
summary(lmTrain) # summary stats/coefficients for model
abline(lmTrain, col = "blue",lwd = 2, lty = "dashed") # plot the regression line
legend("topleft", bty="n",fill=c("black","blue"),cex = .75,
       legend =c("True Relationship","Linear Regression Fit"))

# Q1.4 KNN

df.fx <- data.frame(x.train = sort(x.train)) # data frame with x you want f(x) at, sort x to make plots nice.
knn2 = kknn(y.train~x.train,df.train,df.fx, k = 2, kernel = "rectangular")  # create KNN model with k = 2
knn12 = kknn(y.train~x.train,df.train,df.fx, k = 12, kernel = "rectangular") # create KNN model with k = 12
plot(y.train~x.train) # plot same x/y scatter as above
abline(a = 2, b = 1.8,col = "black",lwd = 2) # add in true relationship
lines(df.fx$x.train,knn2$fitted.values,col="red",lwd=2) # add in KNN2 plot of predicted values
lines(df.fx$x.train,knn12$fitted.values,col="green",lwd=2)  # add in KNN12 plot of predicted values

# add legend to plot
legend("topleft", bty="n",fill=c("black","red","green"),cex = .75,
       legend =c("True Relationship","KNN2","KNN12"))

## 1.5
# vector of possible k values (k=2,3,...,15)
df.test <- as.data.frame(cbind(x.test,y.test)) # build data frame for test set data of interest
df.fxtest <- data.frame(x.test = sort(x.test)) # data frame with x you want f(x) at, sort x to make plots nice.

colnames(df.test)[1:2] <- c("x","y")

kvec=2:15; nk=length(kvec)
outMSE = rep(0,nk) #will will put the out-of-sample MSE here

for(i in 1:nk) {
  near = kknn(y~x,df.train,df.test,k=kvec[i],kernel = "rectangular")
  MSE = mean((df.test$y-near$fitted)^2)
  outMSE[i] = MSE
}

#plot Mean Squared Error against log(1/k) for k = 2,3,...,15
plot(log(1/kvec),outMSE,ylab="Mean Squared Error",ylim = c(1,1.7))
imin = which.min(outMSE)
cat("best k is ",kvec[imin],"\n")

# add in horizontal dashed line representing test set mean squared error using linear regression
# predict test set "Y" using linear model from training set data

lmTestPred <- predict(lmTrain,newdata = df.test)
# calculate LM MSE
lmMSE <- mean((df.test$y- lmTestPred)^2)
abline(h=lmMSE,lwd = 2,lty = "dashed")

# answer here is that linear model performs much better than any K for a KNN model (the best K here appears to be k=11 because that is 
# where the OOS MSE is lowest)

#### 1.6 - Different Data Generation Process for 1.1-1.5 above
xtan <- rnorm(100,0,1) # 100 random draws with a mean of 0 and a st.d. of 1
# sample error
etan <- rnorm(100,0,1) # 100 random draws with a mean of 0 and a st.d. of 1 for error term

# different model y = tanh(1.1 Ã— xi) + 2 + ei
ytan <- tanh(1.1 * xtan) +2 + etan

# scatter plot
plot(ytan~xtan)

# create test set
x.test.tan <- rnorm(10000,0,1) # same thing as above, 10,000 draws this time though
e.test.tan <- rnorm(10000,0,1) # " " " "
y.test.tan <- (1.8*x.test + 2 + e.test)

# Ordinary Linear Regression
ord.lin.tan <- lm(ytan ~ xtan) # create a linear regression model for y on x
summary(ord.lin.tan) # summary stats/coefficients for model
abline(ord.lin.tan, col = "blue",lwd = 1) # plot the regression line

train.tan <- as.data.frame(cbind(xtan,ytan)) # build data frame for training data
test.tan <- data.frame(xtan = sort(xtan)) # build data frame of just x values for test data
knn2.tan = kknn(ytan~xtan,train.tan,test.tan, k = 2, kernel = "rectangular")  # create KNN model with k = 2
knn12.tan = kknn(ytan~xtan,train.tan,test.tan, k = 12, kernel = "rectangular") # create KNN model with k = 12
plot(ytan~xtan) # plot same x/y scatter as above
abline(ord.lin.tan, col = "blue",lwd = 1) # add in linear regression equation from above
lines(test.tan$x,knn2$fitted.values,col="red",lwd=2) # add in KNN2 plot of predicted values
lines(test.tan$x,knn12$fitted.values,col="green",lwd=2)  # add in KNN12 plot of predicted values

# add legend to plot
legend("topleft", bty="n",fill=c("blue","red","green"),cex = .75,
       legend =c("Linear Regression","KNN2","KNN12"))
